[package]
name = "rusty-genius-cortex"
version.workspace = true
edition.workspace = true
authors.workspace = true
license.workspace = true
repository.workspace = true
description = "Inference engine interaction layer for rusty-genius"
readme = "../../README.md"
keywords = ["ai", "llm", "llama-cpp", "gguf", "inference"]
categories.workspace = true
publish.workspace = true

[dependencies]
rusty-genius-core = { path = "../core", version = "0.1.3" }
anyhow = "1.0"
async-std = "1.12"
futures = "0.3"
async-trait = "0.1"
llama-cpp-2 = { version = "=0.1.132", optional = true, features = ["sampler"] } 
# Note: assuming llama-cpp-2 version. In a real scenario I'd check crates.io or use a git dep if needed. 
# For this exercise, I'll assume 0.1 is fine or just rely on the user's environment / mock if it fails.
# Actually, the user prompt says "llama-cpp-2 must be optional".

[features]
default = []
real-engine = ["dep:llama-cpp-2"]
metal = ["llama-cpp-2/metal", "real-engine"]
cuda = ["llama-cpp-2/cuda", "real-engine"]
vulkan = ["llama-cpp-2/vulkan", "real-engine"]
