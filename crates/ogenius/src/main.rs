//! # Ogenius: The Voice
//!
//! The `ogenius` CLI provides an interactive chat REPL and an OpenAI-compatible API server,
//! with automatic model downloading from Huggingface.
//!
//! ## Basic Usage
//!
//! ```bash
//! # Download a model
//! ogenius download Qwen/Qwen2.5-1.5B-Instruct
//!
//! # Start interactive chat
//! ogenius chat --model Qwen/Qwen2.5-1.5B-Instruct
//!
//! # Run the API & Web Server (defaults to port 8080)
//! ogenius serve --model Qwen/Qwen2.5-1.5B-Instruct
//! ```

mod api;

use anyhow::Result;
use api::{chat_completions, list_models, ApiState};
use async_std::sync::Mutex;
use clap::{Parser, Subcommand};
use colored::*;
use futures::channel::mpsc;
use futures::sink::SinkExt;
use futures::StreamExt;
use rusty_genius_core::protocol::{
    AssetEvent, BrainstemInput, BrainstemOutput, InferenceConfig, InferenceEvent,
};
use rusty_genius_stem::Orchestrator;
use std::io::{self, Write};
use std::sync::Arc;
use tide_websockets::{Message, WebSocket};

#[derive(Parser)]
#[command(author, version, about, long_about = None)]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// Download a model from HuggingFace
    Download {
        /// HuggingFace model repo (e.g., Qwen/Qwen2.5-1.5B-Instruct)
        repo: String,
    },
    /// Start interactive chat in CLI
    Serve {
        /// HTTP server address
        #[arg(long, default_value = "127.0.0.1:8080")]
        addr: String,
        /// WebSocket server address
        #[arg(long, default_value = "127.0.0.1:8081")]
        ws_addr: String,
        /// Model repository to pre-load
        #[arg(long)]
        model: Option<String>,
        /// Do not open the browser automatically
        #[arg(long)]
        no_open: bool,
        /// Unload model after inactivity (seconds)
        #[arg(long, default_value = "300")]
        unload_after: u64,
        /// Quantization level (e.g. Q4_K_M)
        #[arg(long, default_value = "Q4_K_M")]
        quant: String,
        /// Context size
        #[arg(long, default_value = "2048")]
        context_size: u32,
        /// Show thinking tokens
        #[arg(long, default_value = "true")]
        show_thinking: bool,
    },
    /// Start interactive chat in CLI
    Chat {
        /// Model repository
        #[arg(long, default_value = "Qwen/Qwen2.5-1.5B-Instruct")]
        model: String,
        /// Quantization level
        #[arg(long, default_value = "Q4_K_M")]
        quant: String,
        /// Context size
        #[arg(long, default_value = "2048")]
        context_size: u32,
        /// Show thinking tokens
        #[arg(long, default_value = "true")]
        show_thinking: bool,
    },
    /// Generate embeddings for input text
    Embed {
        /// Model repository
        #[arg(long, default_value = "Qwen/Qwen2.5-1.5B-Instruct")]
        model: String,
        /// Quantization level
        #[arg(long, default_value = "Q4_K_M")]
        quant: String,
        /// Input text to embed
        input: String,
        /// Context size
        #[arg(long, default_value = "2048")]
        context_size: u32,
    },
}

#[async_std::main]
async fn main() -> anyhow::Result<()> {
    let cli = Cli::parse();

    match cli.command {
        Commands::Download { repo } => {
            println!("üì• Downloading {}", repo.cyan());
            let mut orchestrator = Orchestrator::new().await?;
            let (mut input_tx, input_rx) = mpsc::channel(10);
            let (output_tx, mut output_rx) = mpsc::channel(10);

            async_std::task::spawn(async move {
                let _ = orchestrator.run(input_rx, output_tx).await;
            });

            input_tx.send(BrainstemInput::LoadModel(repo)).await?;

            while let Some(output) = output_rx.next().await {
                match output {
                    BrainstemOutput::Asset(AssetEvent::Progress(curr, total)) => {
                        let pct = if total > 0 {
                            (curr as f64 / total as f64) * 100.0
                        } else {
                            0.0
                        };
                        print!("\rProgress: {:.1}% ({}/{})", pct, curr, total);
                        io::stdout().flush()?;
                    }
                    BrainstemOutput::Asset(AssetEvent::Complete(path)) => {
                        println!("\n‚úÖ Download complete: {}", path.green());
                        break;
                    }
                    BrainstemOutput::Asset(AssetEvent::Error(e)) => {
                        eprintln!("\n‚ùå Error: {}", e.red());
                        break;
                    }
                    BrainstemOutput::Error(e) => {
                        eprintln!("\n‚ùå Orchestrator Error: {}", e.red());
                        break;
                    }
                    _ => {}
                }
            }
        }
        Commands::Chat {
            model,
            quant: _,
            context_size,
            show_thinking,
        } => {
            println!("üí¨ Starting chat with {}", model.cyan());
            let mut orchestrator = Orchestrator::new().await?;
            let (mut input_tx, input_rx) = mpsc::channel(10);
            let (output_tx, mut output_rx) = mpsc::channel(10);

            async_std::task::spawn(async move {
                let _ = orchestrator.run(input_rx, output_tx).await;
            });

            let config = InferenceConfig {
                context_size: Some(context_size),
                show_thinking,
                ..Default::default()
            };

            // Pre-load model
            input_tx
                .send(BrainstemInput::LoadModel(model.clone()))
                .await?;
            println!("‚è≥ Loading model...");

            while let Some(output) = output_rx.next().await {
                match output {
                    BrainstemOutput::Asset(AssetEvent::Complete(_)) => break,
                    BrainstemOutput::Asset(AssetEvent::Progress(0, _)) => {
                        // Just waiting
                    }
                    BrainstemOutput::Error(e) => {
                        eprintln!("‚ùå Failed to load: {}", e.red());
                        return Ok(());
                    }
                    _ => {}
                }
            }
            println!("‚úÖ Ready!");

            loop {
                print!("{} ", "You >".bright_blue());
                io::stdout().flush()?;
                let mut input = String::new();
                if io::stdin().read_line(&mut input)? == 0 {
                    break;
                }
                let prompt = input.trim();
                if prompt.is_empty() {
                    continue;
                }

                input_tx
                    .send(BrainstemInput::Infer {
                        model: Some(model.clone()),
                        prompt: prompt.to_string(),
                        config: config.clone(),
                    })
                    .await?;

                print!("{} ", "AI >".bright_green());
                io::stdout().flush()?;

                while let Some(output) = output_rx.next().await {
                    match output {
                        BrainstemOutput::Event(InferenceEvent::Content(c)) => {
                            print!("{}", c);
                            io::stdout().flush()?;
                        }
                        BrainstemOutput::Event(InferenceEvent::Complete) => {
                            println!();
                            break;
                        }
                        BrainstemOutput::Error(e) => {
                            eprintln!("\n‚ùå Error: {}", e.red());
                            break;
                        }
                        _ => {}
                    }
                }
            }
        }
        Commands::Embed {
            model,
            quant: _,
            input,
            context_size,
        } => {
            println!("üî¢ Generating embeddings using {}", model.cyan());
            let mut orchestrator = Orchestrator::new().await?;
            let (mut input_tx, input_rx) = mpsc::channel(10);
            let (output_tx, mut output_rx) = mpsc::channel(10);

            async_std::task::spawn(async move {
                let _ = orchestrator.run(input_rx, output_tx).await;
            });

            let config = InferenceConfig {
                context_size: Some(context_size),
                show_thinking: false,
                ..Default::default()
            };

            // Pre-load model
            input_tx
                .send(BrainstemInput::LoadModel(model.clone()))
                .await?;
            println!("‚è≥ Loading model...");

            while let Some(output) = output_rx.next().await {
                match output {
                    BrainstemOutput::Asset(AssetEvent::Complete(_)) => break,
                    BrainstemOutput::Error(e) => {
                        eprintln!("‚ùå Failed to load: {}", e.red());
                        return Ok(());
                    }
                    _ => {}
                }
            }
            println!("‚úÖ Model loaded!");

            // Send embedding request
            input_tx
                .send(BrainstemInput::Embed {
                    model: Some(model),
                    input,
                    config,
                })
                .await?;

            println!("‚è≥ Generating embedding...");

            while let Some(output) = output_rx.next().await {
                match output {
                    BrainstemOutput::Event(InferenceEvent::Embedding(emb)) => {
                        println!("‚úÖ Embedding generated ({} dimensions)", emb.len());
                        println!("First 10 values: {:?}", &emb[..10.min(emb.len())]);
                        break;
                    }
                    BrainstemOutput::Event(InferenceEvent::Complete) => {
                        break;
                    }
                    BrainstemOutput::Error(e) => {
                        eprintln!("‚ùå Error: {}", e.red());
                        break;
                    }
                    _ => {}
                }
            }
        }
        Commands::Serve {
            addr,
            ws_addr,
            model,
            no_open,
            unload_after: _,
            quant: _,
            context_size,
            show_thinking,
        } => {
            println!("üöÄ Starting server at {}", addr.cyan());
            println!("üîå WebSocket endpoint at {}", ws_addr.cyan());

            let mut orchestrator = Orchestrator::new().await?;
            let (input_tx, input_rx) = mpsc::channel(100);
            let (output_tx, mut output_rx) = mpsc::channel(100);

            // Senders for all active WebSocket clients
            let ws_senders: Arc<Mutex<Vec<mpsc::Sender<BrainstemOutput>>>> =
                Arc::new(Mutex::new(Vec::new()));

            // API output channel for chat_completions
            let (api_output_tx, api_output_rx) = mpsc::channel(100);
            let api_output_rx = Arc::new(Mutex::new(api_output_rx));

            let state = ApiState {
                input_tx: input_tx.clone(),
                output_rx: api_output_rx,
                ws_addr: ws_addr.clone(),
            };

            async_std::task::spawn(async move {
                let _ = orchestrator.run(input_rx, output_tx).await;
            });

            // Bridge orchestrator output to WS senders and API channel
            let ws_senders_bridge = ws_senders.clone();
            let mut api_output_tx_clone = api_output_tx.clone();
            async_std::task::spawn(async move {
                while let Some(msg) = output_rx.next().await {
                    // Send to API
                    let _ = api_output_tx_clone.send(msg.clone()).await;

                    // Broadcast to all WS clients
                    let mut senders = ws_senders_bridge.lock().await;
                    let mut to_remove = Vec::new();
                    for (i, sender) in senders.iter_mut().enumerate() {
                        if sender.send(msg.clone()).await.is_err() {
                            to_remove.push(i);
                        }
                    }
                    // Clean up closed senders
                    for i in to_remove.into_iter().rev() {
                        senders.remove(i);
                    }
                }
            });

            let inference_config = InferenceConfig {
                context_size: Some(context_size),
                show_thinking,
                ..Default::default()
            };

            if let Some(m) = model {
                println!("‚è≥ Pre-loading model {}...", m.cyan());
                input_tx.clone().send(BrainstemInput::LoadModel(m)).await?;

                // For pre-load, we can just wait for a Complete event on a temporary receiver
                // But the bridge task consumes all messages.
                // This is a bit tricky. For pre-load, let's just wait a bit or
                // we'd need a more complex event bus.
                // For now, let's just assume pre-load works or use the API output channel's lock.
                // Wait, if we use API output channel, it might steal messages from the bridge? No, bridge consumes from orchestrator.
                // So we should check the BROADCAST or similar.
                // Actually, pre-load happens before server starts listening.
                // Let's just wait for a few seconds or ignore the wait for now.
                println!("‚úÖ Model pre-load triggered!");
            }

            let mut app = tide::with_state(state);

            app.at("/").get(|_| async {
                let html = include_str!("index.html");
                Ok(tide::Response::builder(200)
                    .content_type(tide::http::mime::HTML)
                    .body(html)
                    .build())
            });

            app.at("/v1/models").get(list_models);
            app.at("/v1/chat_completions").post(chat_completions);
            app.at("/v1/embeddings").post(api::embeddings);
            app.at("/v1/config").get(api::get_config);

            // WS server with real streaming
            let input_tx_ws = input_tx.clone();
            let ws_senders_ws = ws_senders.clone();
            let _ws_server = async_std::task::spawn(async move {
                let mut ws_app = tide::new();
                ws_app.at("/").get(WebSocket::new(move |_req, mut stream| {
                    let mut input_tx = input_tx_ws.clone();
                    let ws_senders = ws_senders_ws.clone();
                    let inference_config = inference_config.clone();
                    async move {
                        // Create a channel for THIS client
                        let (tx, mut rx) = mpsc::channel(100);
                        {
                            let mut senders = ws_senders.lock().await;
                            senders.push(tx);
                        }

                        // Spawn a task to forward events from our private channel to WS
                        let stream_clone = stream.clone();
                        async_std::task::spawn(async move {
                            while let Some(event) = rx.next().await {
                                if let Ok(json) = serde_json::to_string(&event) {
                                    if stream_clone.send_string(json).await.is_err() {
                                        break;
                                    }
                                }
                            }
                        });

                        while let Some(Ok(Message::Text(input))) = stream.next().await {
                            if let Ok(json) = serde_json::from_str::<serde_json::Value>(&input) {
                                let prompt = json["prompt"].as_str().unwrap_or("").to_string();
                                let model = json["model"].as_str().map(|s| s.to_string());
                                let _ = input_tx
                                    .send(BrainstemInput::Infer {
                                        model,
                                        prompt,
                                        config: inference_config.clone(),
                                    })
                                    .await;
                            }
                        }
                        Ok(())
                    }
                }));
                let _ = ws_app.listen(ws_addr).await;
            });

            if !no_open {
                let url = if addr.starts_with(':') {
                    format!("http://127.0.0.1{}", addr)
                } else if addr.starts_with('0') {
                    addr.replace("0.0.0.0", "127.0.0.1")
                } else {
                    format!("http://{}", addr)
                };
                let _ = open_browser(&url).await;
            }

            app.listen(addr).await?;
        }
    }

    Ok(())
}

async fn open_browser(url: &str) -> Result<()> {
    #[cfg(target_os = "macos")]
    let cmd = "open";
    #[cfg(target_os = "linux")]
    let cmd = "xdg-open";
    #[cfg(target_os = "windows")]
    let cmd = "start";

    #[cfg(not(target_os = "windows"))]
    let status = async_std::process::Command::new(cmd)
        .arg(url)
        .status()
        .await?;

    #[cfg(target_os = "windows")]
    let status = async_std::process::Command::new("cmd")
        .arg("/c")
        .arg(cmd)
        .arg(url)
        .status()
        .await?;

    if !status.success() {
        eprintln!("‚ö†Ô∏è Failed to open browser: {}", url);
    }

    Ok(())
}
